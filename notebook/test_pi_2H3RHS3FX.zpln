{
  "paragraphs": [
    {
      "text": "%pyspark\nimport sys\nfrom random import random\nfrom operator import add\n\nfrom pyspark.sql import SparkSession\n\n\nif __name__ \u003d\u003d \"__main__\":\n    spark \u003d SparkSession\\\n        .builder\\\n        .appName(\"PythonPi\")\\\n        .getOrCreate()\n    # Set log4j\n    spark.sparkContext.setLogLevel(\"ERROR\")\n    log4jLogger \u003d spark._jvm.org.apache.log4j\n    logger \u003d log4jLogger.LogManager.getLogger(\"LOGGER\")\n    logger.setLevel(log4jLogger.Level.INFO)\n\n    partitions \u003d int(sys.argv[1]) if len(sys.argv) \u003e 1 else 2\n    n \u003d 100000 * partitions\n\n    def f(_: int) -\u003e float:\n        x \u003d random() * 2 - 1\n        y \u003d random() * 2 - 1\n        return 1 if x ** 2 + y ** 2 \u003c\u003d 1 else 0\n\n    count \u003d spark.sparkContext.parallelize(\n        range(1, n + 1), partitions).map(f).reduce(add)\n    print(\"Pi is roughly %f\" % (4.0 * count / n))\n\n    spark.stop()\n",
      "user": "admin",
      "dateUpdated": "2022-04-26 11:31:48.065",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Fail to launch interpreter process:\nInterpreter download command: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-shared_process--77d00480357d.log -cp :/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader 192.168.2.2 40057 spark /opt/zeppelin/local-repo/spark\n INFO [2022-04-26 11:31:48,623] ({main} RemoteInterpreterDownloader.java[syncAllLibraries]:73) - Loading all libraries for interpreter spark to /opt/zeppelin/local-repo/spark\n[INFO] Interpreter launch command: /opt/zeppelin/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path :/opt/zeppelin/local-repo/spark/*:/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar --driver-java-options  -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-shared_process--77d00480357d.log --conf spark.kubernetes.container.image\u003d192.168.0.1:5000/main-spark:latest --conf spark.executor.instances\u003d2 --conf spark.app.name\u003dspark-shared_process --conf spark.webui.yarn.useProxy\u003dfalse --conf spark.driver.cores\u003d2 --conf spark.kubernetes.authenticate.submission.oauthToken\u003deyJhbGciOiJSUzI1NiIsImtpZCI6IkVvQ2dFUmJmMUFNSmlkYWdyeTBldElSMzRtUTctUXp2RW1ETkc0Z3JpQ0UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InNwYXJrLXRva2VuLTV4eDlqIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InNwYXJrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjRkZDRiYmMtZjBkMC00ZWVhLTk0MjctYzZiNzY0NTY0YTE5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6c3BhcmsifQ.Fn2sLxG66ygfxCdXGIqKjdq1R1TmpdIF-FC4l-K0HluAGDY8V5kBCQ2Gcfej1fgcVE5AjlRfMxP40AJC6cs-a7oP9N6hvJ_ke-13mGcN5Ea2iy2Y_O2choXqC3jT55aLtYaClFQqKCLt4i8Rfj97U0cW0UCJ0h3ZaiZeGpOYYSDEQtvzijVKR-RGTu0PrGmN1DFQTb67PhSyDkh5L3CPHeKN8l3PMXkEl2vAZPcYSPGnaH3MNPGahD9TN7PpkD8tmFi7xS2zB7MJPAHScTo9deawzL_hXUuZ1bkeihRC8gGGVm0pSHYUP7Cc98maDF_MLJdrpJXhTdPew8igs8k3Vg --conf spark.executor.memory\u003d2g --conf spark.master\u003dk8s://https://192.168.0.1:6443 --conf spark.kubernetes.container.image.pullPolicy\u003dAlways --conf spark.driver.memory\u003d2g --conf spark.submit.deployMode\u003dcluster --conf spark.executor.cores\u003d2 --conf spark.kubernetes.authenticate.driver.serviceAccountName\u003dspark --conf spark.kubernetes.authenticate.submission.caCertFile\u003d/opt/zeppelin/spark/k8s_conf/certificate.pem /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar 192.168.2.2 40057 spark-shared_process :\n22/04/26 11:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark\u0027s default log4j profile: org/apache/spark/log4j-defaults.properties\n22/04/26 11:31:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n22/04/26 11:31:50 WARN Config: Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.\n22/04/26 11:31:50 WARN Config: Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.\n22/04/26 11:31:51 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.\nException in thread \"main\" org.apache.spark.SparkException: Please specify spark.kubernetes.file.upload.path property.\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:330)\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:276)\n\tat scala.collection.immutable.ArraySeq.$anonfun$map$1(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq.$anonfun$map$1$adapted(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq$.tabulate(ArraySeq.scala:286)\n\tat scala.collection.immutable.ArraySeq$.tabulate(ArraySeq.scala:265)\n\tat scala.collection.ClassTagIterableFactory$AnyIterableDelegate.tabulate(Factory.scala:679)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:275)\n\tat org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatureStep.scala:173)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:164)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$4(KubernetesDriverBuilder.scala:65)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:63)\n\tat org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:106)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$4(KubernetesClientApplication.scala:220)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$4$adapted(KubernetesClientApplication.scala:214)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2713)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:214)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:186)\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/04/26 11:31:51 INFO ShutdownHookManager: Shutdown hook called\n22/04/26 11:31:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-76305fe1-62e8-4cb2-81ba-7b45c924ad96\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:129)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:271)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:440)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:71)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Fail to launch interpreter process:\nInterpreter download command: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-shared_process--77d00480357d.log -cp :/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader 192.168.2.2 40057 spark /opt/zeppelin/local-repo/spark\n INFO [2022-04-26 11:31:48,623] ({main} RemoteInterpreterDownloader.java[syncAllLibraries]:73) - Loading all libraries for interpreter spark to /opt/zeppelin/local-repo/spark\n[INFO] Interpreter launch command: /opt/zeppelin/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path :/opt/zeppelin/local-repo/spark/*:/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.0.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar --driver-java-options  -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dfile:///opt/zeppelin/conf/log4j.properties -Dlog4j.configurationFile\u003dfile:///opt/zeppelin/conf/log4j2.properties -Dzeppelin.log.file\u003d/opt/zeppelin/logs/zeppelin-interpreter-spark-shared_process--77d00480357d.log --conf spark.kubernetes.container.image\u003d192.168.0.1:5000/main-spark:latest --conf spark.executor.instances\u003d2 --conf spark.app.name\u003dspark-shared_process --conf spark.webui.yarn.useProxy\u003dfalse --conf spark.driver.cores\u003d2 --conf spark.kubernetes.authenticate.submission.oauthToken\u003deyJhbGciOiJSUzI1NiIsImtpZCI6IkVvQ2dFUmJmMUFNSmlkYWdyeTBldElSMzRtUTctUXp2RW1ETkc0Z3JpQ0UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InNwYXJrLXRva2VuLTV4eDlqIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InNwYXJrIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjRkZDRiYmMtZjBkMC00ZWVhLTk0MjctYzZiNzY0NTY0YTE5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6c3BhcmsifQ.Fn2sLxG66ygfxCdXGIqKjdq1R1TmpdIF-FC4l-K0HluAGDY8V5kBCQ2Gcfej1fgcVE5AjlRfMxP40AJC6cs-a7oP9N6hvJ_ke-13mGcN5Ea2iy2Y_O2choXqC3jT55aLtYaClFQqKCLt4i8Rfj97U0cW0UCJ0h3ZaiZeGpOYYSDEQtvzijVKR-RGTu0PrGmN1DFQTb67PhSyDkh5L3CPHeKN8l3PMXkEl2vAZPcYSPGnaH3MNPGahD9TN7PpkD8tmFi7xS2zB7MJPAHScTo9deawzL_hXUuZ1bkeihRC8gGGVm0pSHYUP7Cc98maDF_MLJdrpJXhTdPew8igs8k3Vg --conf spark.executor.memory\u003d2g --conf spark.master\u003dk8s://https://192.168.0.1:6443 --conf spark.kubernetes.container.image.pullPolicy\u003dAlways --conf spark.driver.memory\u003d2g --conf spark.submit.deployMode\u003dcluster --conf spark.executor.cores\u003d2 --conf spark.kubernetes.authenticate.driver.serviceAccountName\u003dspark --conf spark.kubernetes.authenticate.submission.caCertFile\u003d/opt/zeppelin/spark/k8s_conf/certificate.pem /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.0.jar 192.168.2.2 40057 spark-shared_process :\n22/04/26 11:31:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark\u0027s default log4j profile: org/apache/spark/log4j-defaults.properties\n22/04/26 11:31:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n22/04/26 11:31:50 WARN Config: Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.\n22/04/26 11:31:50 WARN Config: Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.\n22/04/26 11:31:51 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.\nException in thread \"main\" org.apache.spark.SparkException: Please specify spark.kubernetes.file.upload.path property.\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:330)\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:276)\n\tat scala.collection.immutable.ArraySeq.$anonfun$map$1(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq.$anonfun$map$1$adapted(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq$.tabulate(ArraySeq.scala:286)\n\tat scala.collection.immutable.ArraySeq$.tabulate(ArraySeq.scala:265)\n\tat scala.collection.ClassTagIterableFactory$AnyIterableDelegate.tabulate(Factory.scala:679)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:71)\n\tat scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\n\tat org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:275)\n\tat org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatureStep.scala:173)\n\tat scala.collection.immutable.List.foreach(List.scala:333)\n\tat org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:164)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$4(KubernetesDriverBuilder.scala:65)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:63)\n\tat org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:106)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$4(KubernetesClientApplication.scala:220)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$4$adapted(KubernetesClientApplication.scala:214)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2713)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:214)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:186)\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/04/26 11:31:51 INFO ShutdownHookManager: Shutdown hook called\n22/04/26 11:31:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-76305fe1-62e8-4cb2-81ba-7b45c924ad96\n\n\tat org.apache.zeppelin.interpreter.remote.ExecRemoteInterpreterProcess.start(ExecRemoteInterpreterProcess.java:97)\n\tat org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:68)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:104)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:154)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:126)\n\t... 13 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1650958943978_758923073",
      "id": "paragraph_1650958943978_758923073",
      "dateCreated": "2022-04-26 07:42:23.978",
      "dateStarted": "2022-04-26 11:31:48.072",
      "dateFinished": "2022-04-26 11:31:51.645",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n",
      "user": "admin",
      "dateUpdated": "2022-04-26 07:42:59.163",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1650958979163_1826462803",
      "id": "paragraph_1650958979163_1826462803",
      "dateCreated": "2022-04-26 07:42:59.163",
      "status": "READY"
    }
  ],
  "name": "test_pi",
  "id": "2H3RHS3FX",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": false
  }
}